{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0efb80a-b16a-4f09-8df0-372852b9b3a6",
   "metadata": {},
   "source": [
    "# <p style=\"text-align:center;\">Tensorflow - VI</p>\n",
    "---\n",
    "*<p style=\"text-align:right;\">Reference: Tensorflow Official Docs</p>*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f7f5b4-6a4d-4df6-bae8-e33c84a2f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88916862-36e4-473f-99d2-f9692e0f1fbf",
   "metadata": {},
   "source": [
    "# Introduction to Modules, Layers & Models\n",
    "## Overview\n",
    "To do machine learning in TensorFlow, you are likely to need to define, save, and restore a model.\n",
    "\n",
    "A model is, abstractly:\n",
    "\n",
    "* A function that computes something on tensors (a forward pass)\n",
    "* Some variables that can be updated in response to training\n",
    "\n",
    "In this guide, you will go below the surface of Keras to see how TensorFlow models are defined. This looks at how TensorFlow collects variables and models, as well as how they are saved and restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4a7af4-bd0a-4bea-9a47-37713be13509",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8f751-60f3-4b57-a1a4-184997d8e759",
   "metadata": {},
   "source": [
    "## 1. Defining models and layers in Tensorflow\n",
    "\n",
    "Most models are made of layers. Layers are functions with a known mathematical structure that can be reused and have trainable varaibles. In Tensorflow, most high-level implementations of layers and models, such as Keras or Sonnet, are built on the same foundational class: `tf.Module`\n",
    "\n",
    "Here's an example of very simple `tf.Module` that operates on a scalar tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0133c94-1413-44a1-b6bd-83df577d68d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleModule(tf.Module):\n",
    "    def __init__(self,name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.a_variable = tf.Variable(5.0, name = 'train_me')\n",
    "        self.non_trainable_variable = tf.Variable(5.0, trainable = False, name = 'do_not_train_me')\n",
    "    def __call__(self,x):\n",
    "        return self.a_variable*x + self.non_trainable_variable #wx + b\n",
    "simple_module = SimpleModule(name = 'simple')\n",
    "\n",
    "simple_module(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d127f3-432b-493e-9242-19003d7f4813",
   "metadata": {},
   "source": [
    "Modules and, by extension, layers are deep-learning terminology for \"objects\": they have internal state, and methods that use that state.\n",
    "\n",
    "There is nothing special about `__call__` except to act like a Python callable; you can invoke your models with whatever functions you wish.\n",
    "\n",
    "You can set the trainability of variables on and off for any reason, including freezing layers and variables during fine-tuning.\n",
    "\n",
    "By subclassing `tf.Module`, any `tf.Variable` ot `tf.Module` instances assigned to this object's properties are automatically collected. This allows you to save and load variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a4cf1a-f1c9-4151-b5af-c257f48b09d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable variables: (<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>,)\n",
      "all trainable variables: (<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>, <tf.Variable 'do_not_train_me:0' shape=() dtype=float32, numpy=5.0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"trainable variables:\", simple_module.trainable_variables)\n",
    "print(\"all trainable variables:\", simple_module.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f85a3-4948-4c89-be9e-809b24a30b03",
   "metadata": {},
   "source": [
    "This is an example of a two-layer linear layer model made out of modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38356056-3fef-48d6-88e5-1efb39ff6fe7",
   "metadata": {},
   "source": [
    "First a dense (linear) layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fa16e0-766c-4454-b82c-f68ab90897a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining what a single perceptron would do -> does relu(w.T * X + b)\n",
    "# and returns it \n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, in_features, out_features, name = None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([in_features,out_features]), name = 'w')\n",
    "        self.b = tf.Variable(tf.zeros([out_features]), name = 'b')\n",
    "    def __call__(self,x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd686bd-3ecd-4c5b-8f5e-d449719ec51a",
   "metadata": {},
   "source": [
    "**Understand the matrix operations**\n",
    "\n",
    "`w` and `b` are initialised inside the `Dense` class as (`in_features`, `out_features`)  and (`out_features`,) matrix respectively, as defined in the class and gets multiplied by a array given by the user when creating an instance of the `SequentialModel` class. \n",
    "\n",
    "In this case for 1st dense layer\n",
    "- we supply a costant [2,2,2]. Call it x\n",
    "- `w` is initalised as a (3,3) matrix with random numbers from normal distribution\n",
    "- b is a (3,1) bias vector filled with zeros\n",
    "- we multiply x and w ((1,3) X (3,3)) to get (1,3) resulting array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3ad7ec-9854-4da0-b409-f264612bb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([3,3]), name = 'W')\n",
    "B = tf.Variable(tf.zeros([3]), name = 'B')\n",
    "X = tf.constant([[2.0,2.0,2.0]])\n",
    "\n",
    "imd = tf.matmul(X,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4b6f60-3f83-4c92-a227-efd15b972d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.8021361  2.0239859  1.3632247]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(imd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bffdb-2fcc-48ac-945d-c4d0fc3bf334",
   "metadata": {},
   "source": [
    "- This resulting array gets added with b ((1,3) + (3,)) to get (1,3) array whose relu is returned. This is `self.dense_1` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe41d841-87dd-41aa-b4cf-9bf9f3fb85e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.8021361  2.0239859  1.3632247]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y = imd + B\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52b111b-c983-445d-9914-656584958bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.        2.0239859 1.3632247]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fin_res = tf.nn.relu(Y)\n",
    "print(fin_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a70e4-57b8-40a3-a657-aa21f91d347a",
   "metadata": {},
   "source": [
    "For next layer, i.e., `self.dense_2` the o/p from `self.dense_1` is the i/p to it. See the `__call__` section. The o/p from `self.dense_1` (say `fin_res`) is a (1,3) vector. The `in_features` and `out_features` are (3,2). Thus for 2nd layer\n",
    "\n",
    "- we supply `fin_res` as i/p. \n",
    "- `w` is initalised as a (3,2) matrix with random numbers from normal distribution\n",
    "- b is a (2,1) bias vector filled with zeros\n",
    "- we multiply `fin_res` and w ((1,3) X (3,2)) to get (1,2) resulting array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cefb7661-1de7-44b2-8fae-35d65fe43303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W_2 = tf.Variable(tf.random.normal([3,2]), name = 'W_2')\n",
    "B_2 = tf.Variable(tf.zeros([2]), name = 'B_2')\n",
    "X_2 = fin_res\n",
    "\n",
    "imd_2 = tf.matmul(X_2,W_2)\n",
    "Y_2 = imd_2 + B_2\n",
    "fin_res_2 = tf.nn.relu(Y_2)\n",
    "print(fin_res_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed42e5-415a-49aa-a533-1a9930764b89",
   "metadata": {},
   "source": [
    "Complete Model, makes 2 layer instances and applies them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c6a7dcd-243e-4c6c-9a52-dfd43c0773c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class SequentialModel(tf.Module):\n",
    "    def __init__(self,name=None):\n",
    "        super().__init__(name=None)\n",
    "        \n",
    "        self.dense_1 = Dense(in_features = 3, out_features = 3)\n",
    "        self.dense_2 = Dense(in_features = 3, out_features = 2)\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "\n",
    "my_model = SequentialModel(name = \"the_model\")\n",
    "print(\"Model results:\", my_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11897252-f713-4b83-adb8-8229d8fad455",
   "metadata": {},
   "source": [
    "`tf.Module` instances will automatically collect, recursively, any `tf.Variable` or `tf.Module` instances assigned to it. This allows you to manage collections of `tf.Modules` with a single model instance, and save and load whole models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8aa1b0-c49f-45e7-a4dd-1237cc820037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodules: (<__main__.Dense object at 0x00000217EF4422B0>, <__main__.Dense object at 0x00000217FA05FD90>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Submodules:\", my_model.submodules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d978e75e-4254-43be-89fc-e707b30bdfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'w:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 0.01059214, -0.16848826,  0.74092203],\n",
      "       [-2.0518894 ,  0.06184238, -0.5676788 ],\n",
      "       [-1.7585919 ,  1.2362754 ,  0.3948643 ]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-1.0259029 ,  2.1589134 ],\n",
      "       [-0.20522311,  0.16202818],\n",
      "       [ 0.23325957, -0.9435805 ]], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for var in my_model.variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0347e2a-1690-4c48-a7e2-0ba655fce825",
   "metadata": {},
   "source": [
    "### 1.1.Waiting to create variables\n",
    "\n",
    "It's convenient in many cases to wait to create variables until you are sure of the input shape. You may have noticed here that you have to define both input and output sizes to the layer. This is so the `w` variable has a known shape and can be allocated.\n",
    "\n",
    "By deferring variable creation to the first time the module is called with a specific input shape, you do not need specify the input size up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc106948-3f00-4131-952e-1d05b6cfbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class flexibleDenseModule(tf.Module):\n",
    "    def __init__(self, out_features, name = None):\n",
    "        super().__init__(name=name)\n",
    "        self.is_built = False\n",
    "        self.out_features = out_features\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        if not self.is_built:\n",
    "            self.w = tf.Variable(tf.random.normal([x.shape[-1], self.out_features]), name = 'w')\n",
    "            self.b = tf.Variable(tf.zeros([self.out_features]), name = 'b')\n",
    "            self.is_built = True\n",
    "        \n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "886060c3-815c-42b6-a4f5-b1dee6afc1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results tf.Tensor([[0.       5.991182]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MySequentialModule(tf.Module):\n",
    "    def __init__(self, name = None):\n",
    "        super().__init__(name = name)\n",
    "        \n",
    "        self.dense_1 = flexibleDenseModule(out_features = 3)\n",
    "        self.dense_2 = flexibleDenseModule(out_features = 2)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "    \n",
    "my_model = MySequentialModule(name = \"The_model\")\n",
    "print(\"Model results\", my_model(tf.constant([[2.0,2.0,2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28170d7-cd92-431a-8d2e-59eb75841e60",
   "metadata": {},
   "source": [
    "This flexibility is why TensorFlow layers often only need to specify the shape of their outputs, such as in `tf.keras.layers.Dense`, rather than both the input and output size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4aa6fc-57bc-4cb0-8879-de300505de96",
   "metadata": {},
   "source": [
    "## 2. Saving Weights\n",
    "\n",
    "You can save a `tf.Module` as both a checkpoint and a SavedModel.\n",
    "\n",
    "Checkpoints are just the weights (that is, the values of the set of variables inside the module and its submodules):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f198366-42da-4469-9421-86a53db3e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_checkpoint'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_path = \"my_checkpoint\"\n",
    "chkpnt = tf.train.Checkpoint(model = my_model)\n",
    "chkpnt.write(chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10abab8-b659-4910-943e-9c4d2a74b475",
   "metadata": {},
   "source": [
    "Checkpoints consist of two kinds of files: the data itself and an index file for metadata. The index file keeps track of what is actually saved and the numbering of checkpoints, while the checkpoint data contains the variable values and their attribute lookup paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2726f-d9af-4def-a268-5e20e5090993",
   "metadata": {},
   "source": [
    "You can look inside a checkpoint to be sure the whole collection of variables is saved, sorted by the Python object that contains them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d65f210-00f6-4621-9346-1b1379cae456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\n",
       " ('model/dense_1/b/.ATTRIBUTES/VARIABLE_VALUE', [3]),\n",
       " ('model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 3]),\n",
       " ('model/dense_2/b/.ATTRIBUTES/VARIABLE_VALUE', [2]),\n",
       " ('model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 2])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.list_variables(chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b520a-96d5-4352-90df-4dd9d052894e",
   "metadata": {},
   "source": [
    "During distributed (multi-machine) training they can be sharded, which is why they are numbered (e.g., '00000-of-00001'). In this case, though, there is only one shard.\n",
    "\n",
    "When you load models back in, you overwrite the values in your Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a42517c8-6985-4210-82cb-4b637f6a8670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x217f9fbd820>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = MySequentialModule()\n",
    "new_checkpoint = tf.train.Checkpoint(model = new_model)\n",
    "new_checkpoint.restore(\"my_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3051cc-d353-4120-973d-c8856c55f47e",
   "metadata": {},
   "source": [
    "## 3. Keras Models and Layers\n",
    "\n",
    "Note that up until this point, there is no mention of Keras. You can build your own high-level API on top of `tf.Module`, and people have.\n",
    "\n",
    "In this section, you will examine how Keras uses `tf.Module`.\n",
    "\n",
    "### 3.1. Keras Layers\n",
    "\n",
    "`tf.keras.layers.Layer` is the base class of all Keras layers, and it inherits from `tf.Module`.\n",
    "\n",
    "You can convert a module into a Keras layer just by swapping out the parent and then changing `__call__` to `call`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bab6fc25-70d8-4de8-83cf-6fbe76eba112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "  # Adding **kwargs to support base Keras layer arguments\n",
    "  def __init__(self, in_features, out_features, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    # This will soon move to the build step; see below\n",
    "    self.w = tf.Variable(\n",
    "      tf.random.normal([in_features, out_features]), name='w')\n",
    "    self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
    "  \n",
    "    def call(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)\n",
    "\n",
    "simple_layer = MyDense(name=\"simple\", in_features=3, out_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629eaef0-8bf5-49f3-ab95-80cf8a7e981f",
   "metadata": {},
   "source": [
    "Keras layers have their own `__call__` that does some bookkeeping described in the next section and then calls `call()`. You should notice no change in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ba711da-b160-4759-892b-0017ff08543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1.2344629 , 0.45570105, 5.897377  ]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_layer([[2.0,2.0, 2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada8cb5-fc70-400c-bbf4-4919081a4e34",
   "metadata": {},
   "source": [
    "### 3.2. The `build` step\n",
    "\n",
    "As noted, it's convenient in many cases to wait to create variables until you are sure of the input shape.\n",
    "\n",
    "Keras layers come with an extra lifecycle step that allows you more flexibility in how you define your layers. This is defined in the build function.\n",
    "\n",
    "`build` is called exactly once, and it is called with the shape of the input. It's usually used to create variables (weights).\n",
    "\n",
    "You can rewrite `MyDense` layer above to be flexible to the size of its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b633a62-e878-4ca4-ad03-a8ed73c5af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleDense(tf.keras.layers.Layer):\n",
    "  # Note the added `**kwargs`, as Keras supports many arguments\n",
    "    def __init__(self, out_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def build(self, input_shape):  # Create the state of the layer (weights)\n",
    "        self.w = tf.Variable(\n",
    "        tf.random.normal([input_shape[-1], self.out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "\n",
    "    def call(self, inputs):  # Defines the computation from inputs to outputs\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Create the instance of the layer\n",
    "flexible_dense = FlexibleDense(out_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3519b-9ba3-4fb8-96ea-cb3fbf12ae78",
   "metadata": {},
   "source": [
    "At this point, the model has not been built, so there are no variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a5e544-e323-4928-8404-435011ebdd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9f9b0-694d-497d-89f9-7878759758d3",
   "metadata": {},
   "source": [
    "Calling the function allocates appropriately-sized variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb1e00fb-245a-43b9-9bab-22a7f2927556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor(\n",
      "[[-0.36299315 -0.9272399   4.8384423 ]\n",
      " [-0.5444897  -1.3908594   7.2576637 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Call it, with predictably random results\n",
    "print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97ef974d-1d02-4068-a5c6-24f3c274c4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'flexible_dense/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 0.27201393, -2.1972196 ,  0.6269824 ],\n",
       "        [-0.47587693,  0.8998526 ,  1.2342409 ],\n",
       "        [ 0.02236642,  0.83374715,  0.557998  ]], dtype=float32)>,\n",
       " <tf.Variable 'flexible_dense/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c13471-096a-489b-9116-802976c6efbb",
   "metadata": {},
   "source": [
    "Since `build` is only called once, inputs will be rejected if the input shape is not compatible with the layer's variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7bc23c5-aba5-4572-9f22-fa74b9c70064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: Exception encountered when calling layer \"flexible_dense\" \"                 f\"(type FlexibleDense).\n",
      "\n",
      "{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul]\n",
      "\n",
      "Call arguments received by layer \"flexible_dense\" \"                 f\"(type FlexibleDense):\n",
      "  â€¢ inputs=tf.Tensor(shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]])))\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622871f-801e-4d9e-a9dd-5ab7a0921b01",
   "metadata": {},
   "source": [
    "Keras layers have a lot more extra features including:\n",
    "\n",
    "- Optional losses\n",
    "- Support for metrics\n",
    "- Built-in support for an optional training argument to differentiate between training and inference use\n",
    "- `get_config` and `from_config` methods that allow you to accurately store configurations to allow model cloning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48d323-7887-410e-ab93-9dc420fa841a",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  3.3. Keras Models\n",
    "\n",
    "You can define your model as nested Keras layers.\n",
    "\n",
    "However, Keras also provides a full-featured model class called `tf.keras.Model`. It inherits from `tf.keras.layers.Layer`, so a Keras model can be used, nested, and saved in the same way as Keras layers. Keras models come with extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n",
    "\n",
    "You can define the SequentialModule from above with nearly identical code, again converting `__call__` to `call()` and changing the parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbd13592-074b-49a7-800a-ee6910eca4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor([[-2.784345  -0.9372672]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MySequentialModel(tf.keras.Model):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense_1 = FlexibleDense(out_features=3)\n",
    "        self.dense_2 = FlexibleDense(out_features=2)\n",
    "    def call(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "\n",
    "# You have made a Keras model!\n",
    "my_sequential_model = MySequentialModel(name=\"the_model\")\n",
    "\n",
    "# Call it on a tensor, with random results\n",
    "print(\"Model results:\", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf47b5-82c6-41e6-85c8-d5b18b44012e",
   "metadata": {},
   "source": [
    "All the same features are available, including tracking variables and submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20420d53-892c-4401-81a0-8896dc3a5f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_sequential_model/flexible_dense_1/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 0.5948258 ,  0.08509963,  0.8485259 ],\n",
       "        [-0.40347844,  0.18530941, -1.0075909 ],\n",
       "        [-0.4186937 ,  0.49814397, -0.12418467]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_1/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[ 0.18443146, -1.071148  ],\n",
       "        [-1.8971117 , -0.8341634 ],\n",
       "        [-0.38054168,  0.25086123]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sequential_model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "baaae474-5a40-4619-91b4-84a29e724cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.FlexibleDense at 0x217f9fbd430>,\n",
       " <__main__.FlexibleDense at 0x217fa3b74f0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sequential_model.submodules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2187f7-cc40-43ef-b120-090ef72ef1b8",
   "metadata": {},
   "source": [
    "If you are constructing models that are simple assemblages of existing layers and inputs, you can save time and space by using the functional API, which comes with additional features around model reconstruction and architecture.\n",
    "\n",
    "Here is the same model with the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "503501c0-3e94-4b7a-8c1c-8256f45ea32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " flexible_dense_3 (FlexibleD  (None, 3)                12        \n",
      " ense)                                                           \n",
      "                                                                 \n",
      " flexible_dense_4 (FlexibleD  (None, 2)                8         \n",
      " ense)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=[3,])\n",
    "\n",
    "x = FlexibleDense(3)(inputs)\n",
    "x = FlexibleDense(2)(x)\n",
    "\n",
    "my_functional_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "my_functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f72eb439-891c-4b68-8fcd-ec12b92c1869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[13.858565, -9.586419]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_functional_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32749fcb-b985-4958-b3d7-4ca8ed418b94",
   "metadata": {},
   "source": [
    "The major difference here is that the input shape is specified up front as part of the functional construction process. The `input_shape` argument in this case does not have to be completely specified; you can leave some dimensions as `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2f4cf-5766-43f0-b939-109992d463f1",
   "metadata": {},
   "source": [
    "## 4. Saving Keras Models\n",
    "\n",
    "Keras models can be checkpointed, and that will look the same as `tf.Module`.\n",
    "\n",
    "Keras models can also be saved with `tf.saved_model.save()`, as they are modules. However, Keras models have convenience methods and other functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86653253-77e0-410b-a414-0248ae717a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exname_of_file\\assets\n"
     ]
    }
   ],
   "source": [
    "my_sequential_model.save(\"exname_of_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c3768-82c8-4109-9fbe-869d51e22d1f",
   "metadata": {},
   "source": [
    "Just as easily they canbe loaded back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17ff70ab-d024-4418-a5d7-4f8045f8db37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\"exname_of_file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
